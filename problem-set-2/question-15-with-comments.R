#This problem involves the Boston data set, which we saw in the lab for this chapter. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors.
library(MASS)
attach(Boston)
?Boston

#a) For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.
f.zn <- lm(crim~zn)
summary(f.zn)
f.indus <- lm(crim~indus)
summary(f.indus)
f.chas <- lm(crim~chas)
summary(f.chas)
f.nox <- lm(crim~nox)
summary(f.nox)
f.rm <- lm(crim~rm)
summary(f.rm)
f.age <- lm(crim~age)
summary(f.age)
f.dis <- lm(crim~dis)
summary(f.dis)
f.rad <- lm(crim~rad)
summary(f.rad)
f.tax <- lm(crim~tax)
summary(f.tax)
f.ptratio <- lm(crim~ptratio)
summary(f.ptratio)
f.black <- lm(crim~black)
summary(f.black)
f.lstat <- lm(crim~lstat)
summary(f.lstat)
f.medv <- lm(crim~medv)
summary(f.medv)
par(mfrow=c(2,2))
plot(f.chas)
#For H0 : β1 = 0 we can reject the null hypothesis in favor of the Ha that there is a relationship between the predictor and response except for the Charles River dummy variable.

#b) Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis H0 : βj = 0?
mrf <- lm(crim~., data=Boston)
summary(mrf)
#While almost all the variables were registering as statistically significant in the simple linear regression, now only a few are significant. We can reject the H0 for zn, dis, rad, black, and medv at 99% confidence and nox and lstat at the 95%.

#c) How do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coefficients from (a) on the x-axis, and the multiple regression coefficients from (b) on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regression model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis.
sr <- vector("numeric",0)
sr <- c(sr, f.zn$coefficient[2])
sr <- c(sr, f.indus$coefficients[2])
sr <- c(sr, f.chas$coefficients[2])
sr <- c(sr, f.nox$coefficient[2])
sr <- c(sr, f.rm$coefficients[2])
sr <- c(sr, f.age$coefficients[2])
sr <- c(sr, f.dis$coefficients[2])
sr <- c(sr, f.rad$coefficients[2])
sr <- c(sr, f.tax$coefficients[2])
sr <- c(sr, f.ptratio$coefficients[2])
sr <- c(sr, f.black$coefficients[2])
sr <- c(sr, f.lstat$coefficients[2])
sr <- c(sr, f.medv$coefficients[2])
mr <- vector("numeric",0)
mr <- c(mr, mrf$coefficients)
mr <- mr[-1]
plot(sr,mr,col="blue")
#As mentioned above, certain predictors are no longer significant in the multiple linear regression. Some predictors were getting credit by being assoicated with stronger predictors. This is the same as the advertising and sales example in the reading. Newspapers was no longer significant in the MLR.

#d) Is there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, fit a model of the form Y = β0 + β1X + β2X^2 + β3X^3 + ε.
nlzn <- lm(crim~zn + I(zn^2) + I(zn^3))
summary(nlzn)
nlindus <- lm(crim~indus + I(indus^2) + I(indus^3))
summary(nlindus)
nlchas <- lm(crim~chas + I(chas^2) + I(chas^3))
summary(nlchas)
nlnox <- lm(crim~nox + I(nox^2) + I(nox^3))
summary(nlnox)
nlrm <- lm(crim~rm + I(rm^2) + I(rm^3))
summary(nlrm)
nlage <- lm(crim~age + I(age^2) + I(age^3))
summary(nlage)
nldis <- lm(crim~dis + I(dis^2) + I(age^3))
summary(nldis)
nlrad <- lm(crim~rad + I(rad^2) + I(rad^3))
nldis <- lm(crim~dis + I(dis^2) + I(dis^3))
summary(nlrad)
summary(nldis)
nltax <- lm(crim~tax + I(tax^2) + I(tax^3))
summary(nltax)
nlptratio <- lm(crim~ptratio + I(ptratio^2) + I(ptratio^3))
summary(nlptratio)
nlblack <- lm(crim~black + I(black^2) + I(black^3))
summary(nlblack)
nllstat <- lm(crim~lstat + I(lstat^2) + I(lstat^3))
summary(nllstat)
nlmedv <- lm(crim~medv + I(medv^2) + I(medv^3))
summary(nlmedv)
#Indus, nox, age, dis, ptratio, and medv appear to be non-linear relationships because the squared and cubed terms are statistically significant. 

